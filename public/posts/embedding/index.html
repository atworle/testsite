<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Word Embedding | Antonio Thomas Worley</title>
<meta name="keywords" content="">
<meta name="description" content="          Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.1 The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.">
<meta name="author" content="">
<link rel="canonical" href="https://example.org/posts/embedding/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://example.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://example.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://example.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://example.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://example.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://example.org/posts/embedding/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://example.org/posts/embedding/">
  <meta property="og:site_name" content="Antonio Thomas Worley">
  <meta property="og:title" content="Word Embedding">
  <meta property="og:description" content=" Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.1 The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-29T07:29:52-05:00">
    <meta property="article:modified_time" content="2025-01-29T07:29:52-05:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word Embedding">
<meta name="twitter:description" content="          Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.1 The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://example.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Embedding",
      "item": "https://example.org/posts/embedding/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Word Embedding",
  "name": "Word Embedding",
  "description": " Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.1 The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.\n",
  "keywords": [
    
  ],
  "articleBody": " Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.1 The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.\nIn essence, you train a model and it ‘learns’ how to score words based on an ‘arbitrary number of characteristics’, which are often called dimensions.2 Dimensions are basically abstract features derived from word co-occurrence and contextuality. Then you can plot the results on a two dimensional graph and it will show different relationships between the words based on the way they are positioned. Credit: Schmidt.Word\nHowever, the model does not ‘know’ the words, it is simply calculating co-occurrence relationships. So even though gold and golden are similar words, they may appear spatially far from each other on a word embedded grid based on their usage. There are various ways to numerically represent the distances between the words. One common way is to use cosine similarity which is simply a calculation of the angle between the words. Also, the model is calculating more than two dimensions of relationships, so while the visualization helps in making sense of the results, a two dimensional grid is somewhat obscuring the process underneath it.\nThe potential for a technique such as this is limited by one’s creativity. Because word embedded models capture how words are used by tracking the words that appear near each other, many types of computational analysis can benefit from it. As a starting point, word embedded models can aid in tracking semantic shifts overtime. An example would be words like horse and carriage being near travel in the 19th century then car and plane being near it in the 20th century. Also it can be used to just simply look at word associations and generate thematic clusters. For example, you could give a model a bunch of food and it would return words used similarly to those foods.3\n“Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers” uses word embedded models to discover semantic shifts over time in nineteenth-century abolitionist newspapers. They aimed to ‘amplify’ the contributions of black people, women in particular, to the abolitionist cause.4 They theorize that the introduction of new concepts, reframing of existing ones, and advancement and circulation of both of these things can be identified and tracked temporally.5 They essentially connected words to their contexts then connected this to specific newspapers. In essence, they trained a model to learn how a word is used in context then separated their results over time. With these results, they built a network to see which newspapers were leading these changes and which were following. They discovered that there was a ‘lexical semantic leadership’ of newspapers and the way the language was used in the identified leaders shaped the language of other abolitionists newspapers.\nIn “Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, Charles Romney uses an older form of word embedding called ‘word context vector space modelling’ to identify the ‘creative adaptation of imperial law’.6 He embedded the Kingdom of Hawaii’s legal decisions in a word-context vector space model in order to determine if the contexts of legal decisions concerning Habeas Corpus were semantically similar. His method consisted of generating a five word key-word-in-context then creating a document term matrix where the columns were decisions and the rows were words. Using cosine similarity he was able to determine decisions with similar language use. For example, in one decision he discovered that the word labor had a stable and developed legal vocabulary surrounding it. Based on their cosine similarity the word labor showed up in multiple Habeas Corpus cases. This kind of insight would have been difficult without using word vector space modeling. Romney ultimately determined that judges describing Habeas Corpus had consistent semantic vocabularies on labor and liberty but their decisions were fluid based on the status of the petitioner.7\nThe potential for this method is endless and I have plenty of ideas for it. One idea would be similar to the Abolitionist Networks piece where I attempt to track semantic shifts in 18th century newspapers. This would be especially politically charged words like tax, tyranny, Britain, royalty, or even just politics. The goal would be to pinpoint when semantic shifts surrounding these words happened. So for example if I trained a model on the word politics then fed it newspapers by year, I could determine when shifts around the word politics happened. Not only are the insights from the word embeddings useful but the potential for further inquiry is great. For example, when shifts happened, in what papers did they happen, and what events potentially drove the shifts are all relevant and revealing questions to answer. Specifically the word tyranny would be fascinating because of its contemporary meaning. As an example, if a shift happened prior to 1765 it could be historically significant because the Stamp Tax is often attributed with igniting an intense discourse around British authority and injecting the word tyranny into political discourse.\nBibliography Romney, Charles, W. ”Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, 1852–92. Law and History Review. 2016;34(4):999-1026. doi:10.1017/S0738248016000353\nSchmidt, Ben. “Word Embeddings for the Digital Humanities,” Sapping Attention (blog), October 25, 2015, https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html.\nSoni, Sandeep, Lauren F. Klein, and Jacob Eisenstein. 2021. “Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers.” Journal of Cultural Analytics 6 (1). https:/​/​doi.org/​10.22148/​001c.18841\nBen Schmidt, “Word Embeddings for the Digital Humanities,” Sapping Attention (blog), October 25, 2015, https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html. ↩︎\nSchmidt. Word ↩︎\nSchmidt. Word ↩︎\nSandeep Soni, Lauren F. Klein, and Jacob Eisenstein. 2021. “Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers.” Journal of Cultural Analytics 6 (1). https:/​/​doi.org/​10.22148/​001c.18841. 3 ↩︎\nSoni. Abolitionist. 3 ↩︎\nRomney, Charles, W. ”Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, 1852–92. Law and History Review. 2016;34(4):999-1026. doi:10.1017/S0738248016000353 1002 ↩︎\nRomney. Vector. 1025 ↩︎\n",
  "wordCount" : "1029",
  "inLanguage": "en",
  "datePublished": "2025-01-29T07:29:52-05:00",
  "dateModified": "2025-01-29T07:29:52-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.org/posts/embedding/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Antonio Thomas Worley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://example.org/" accesskey="h" title="Antonio Thomas Worley (Alt + H)">Antonio Thomas Worley</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv" title="CV">
                    <span>CV</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/Searchpage" title="Search Page">
                    <span>Search Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Word Embedding
    </h1>
    <div class="post-meta"><span title='2025-01-29 07:29:52 -0500 EST'>January 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>          Word embedded models are a complex digital method to employ. Conceptually, Ben Schmidt states word embedded models are a ‘spatial analogy’ for relationships between words.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> The philosophy behind word embedded models is the question “What words are like other words”? Not necessarily as synonyms but rather in terms of their usage. Instead of simply counting words, a word embedded model computationally models word similarities based on similar semantic usage. The philosophy behind it becomes easier to understand after looking at how the method works in practice.</p>
<p>          In essence, you train a model and it ‘learns’ how to score words based on an ‘arbitrary number of characteristics’, which are often called dimensions.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Dimensions are basically abstract features derived from word co-occurrence and contextuality. Then you can plot the results on a two dimensional graph and it will show different relationships between the words based on the way they are positioned.
<img alt="exampleofwordembeddingfrombenschmidt" loading="lazy" src="/schmidt.png"> <em>Credit: Schmidt.Word</em></p>
<p>However, the model does not ‘know’ the words, it is simply calculating co-occurrence relationships. So even though gold and golden are similar words, they may appear spatially far from each other on a word embedded grid based on their usage. There are various ways to numerically represent the distances between the words. One common way is to use cosine similarity which is simply a calculation of the angle between the words. Also, the model is calculating more than two dimensions of relationships, so while the visualization helps in making sense of the results, a two dimensional grid is somewhat obscuring the process underneath it.</p>
<p>          The potential for a technique such as this is limited by one’s creativity. Because word embedded models capture how words are used by tracking the words that appear near each other, many types of computational analysis can benefit from it. As a starting point, word embedded models can aid in tracking semantic shifts overtime. An example would be words like horse and carriage being near travel in the 19th century  then car and plane being near it in the 20th century. Also it can be used to just simply look at word associations and generate thematic clusters. For example, you could give a model a bunch of food and it would return words used similarly to those foods.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>          “Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers” uses word embedded models to discover semantic shifts over time in nineteenth-century abolitionist newspapers. They aimed to ‘amplify’ the contributions of black people, women in particular, to the abolitionist cause.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> They theorize that the introduction of new concepts, reframing of existing ones, and advancement and circulation of both of these things can be identified and tracked temporally.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> They essentially connected words to their contexts then connected this to specific newspapers. In essence, they trained a model to learn how a word is used in context then separated their results over time. With these results, they built a network to see which newspapers were leading these changes and which were following. They discovered that there was a ‘lexical semantic leadership’ of newspapers and the way the language was used in the identified leaders shaped the language of other abolitionists newspapers.</p>
<p>          In “Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, Charles Romney uses an older form of word embedding called ‘word context vector space modelling’ to identify the ‘creative adaptation of imperial law’.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> He embedded the Kingdom of Hawaii’s legal decisions in a word-context vector space model in order to determine if the contexts of legal decisions concerning Habeas Corpus were semantically similar. His method consisted of generating a five word key-word-in-context then creating a document term matrix where the columns were decisions and the rows were words. Using cosine similarity he was able to determine decisions with similar language use. For example, in one decision he discovered that the word labor had a stable and developed legal vocabulary surrounding it. Based on their cosine similarity the word labor showed up in multiple Habeas Corpus cases. This kind of insight would have been difficult without using word vector space modeling. Romney ultimately determined that judges describing Habeas Corpus had consistent semantic vocabularies on labor and liberty but their decisions were fluid based on the status of the petitioner.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<p>          The potential for this method is endless and I have plenty of ideas for it. One idea would be similar to the Abolitionist Networks piece where I attempt to track semantic shifts in 18th century newspapers. This would be especially politically charged words like tax, tyranny, Britain, royalty, or even just politics. The goal would be to pinpoint when semantic shifts surrounding these words happened. So for example if I trained a model on the word politics then fed it newspapers by year, I could determine when shifts around the word politics happened. Not only are the insights from the word embeddings useful but the potential for further inquiry is great. For example, when shifts happened, in what papers did they happen, and what events potentially drove the shifts are all relevant and revealing questions to answer. Specifically the word tyranny would be fascinating because of its contemporary meaning. As an example, if a shift happened prior to 1765 it could be historically significant because the Stamp Tax is often attributed with igniting an intense discourse around British authority and injecting the word tyranny into political discourse.</p>
<h2 id="bibliography">Bibliography<a hidden class="anchor" aria-hidden="true" href="#bibliography">#</a></h2>
<p>Romney, Charles, W. ”Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, 1852–92. Law and History Review. 2016;34(4):999-1026. doi:10.1017/S0738248016000353</p>
<p>Schmidt, Ben. “Word Embeddings for the Digital Humanities,” Sapping Attention (blog), October 25, 2015, <a href="https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html</a>.</p>
<p>Soni, Sandeep, Lauren F. Klein, and Jacob Eisenstein. 2021. “Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers.” Journal of Cultural Analytics 6 (1). https:/​/​doi.org/​10.22148/​001c.18841</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ben Schmidt, “Word Embeddings for the Digital Humanities,” Sapping Attention (blog), October 25, 2015, <a href="https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">https://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Schmidt. Word&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Schmidt. Word&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Sandeep Soni,  Lauren F. Klein, and Jacob Eisenstein. 2021. “Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers.” Journal of Cultural Analytics 6 (1). https:/​/​doi.org/​10.22148/​001c.18841. 3&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Soni. Abolitionist. 3&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Romney, Charles, W. ”Using Vector Space Models to Understand the Circulation of Habeas Corpus in Hawai’i”, 1852–92. Law and History Review. 2016;34(4):999-1026. doi:10.1017/S0738248016000353 1002&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Romney. Vector. 1025&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://example.org/">Antonio Thomas Worley</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

